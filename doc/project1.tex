\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
% For figures and graphics'n stuff
\usepackage{graphicx}
\usepackage{caption}

% \usepackage{subfigure}
\usepackage{subcaption}

\usepackage{tabularx}
\usepackage{url}
\usepackage{color}
\usepackage{float}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{bm}

\usepackage[toc,page]{appendix}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% \usepackage{fancyhdr}

% For customized hlines in tables
\usepackage{ctable}



% Correct way for typing C++
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}

% Remembrance 
\newcommand{\husk}[1]{\color{red}#1\color{black}}

% For cmd line arguments
\usepackage{listings}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt}


\lstdefinestyle{custom-pro-file}{
  % belowcaptionskip=1\baselineskip,
  % frame=L,
  % xleftmargin=\parindent,
  % language=C++,
  basicstyle=\footnotesize\ttfamily,
  % commentstyle=\itshape\color{green!40!black},
  % keywordstyle=\bfseries\color{purple!40!black},
  identifierstyle=\color{black},
}


% For proper citations
% \usepackage[round, authoryear]{natbib}
\usepackage[numbers]{natbib} 

% For color
\hypersetup{colorlinks=true,linkcolor=blue, linktocpage}

% For fixing large table height
\usepackage{a4wide}

\title{Regression analysis on terrain data}
\author{Mathias M. Vege}

\date{\today}
\begin{document}
\maketitle

\begin{abstract}
An often used tool in Machine Learning is that of Linear Regression and resampling. For this paper, we investigated how to use Ordinary Least Squares, Ridge and Lasso regression together with resampling techniques such as bootstrapping, $k$-fold Cross Validation and Monte Carlo Cross Validation. After first studying the Franke function and using that as a proving ground for the different models, we moved on to study terrain data from a region outside Stavanger. The optimal model for this region turned out to be OLS with $R^2=0.91$ and MSE$=1.26\cdot 10^{-3}$, which is an acceptable result considering the scope of this report.
\end{abstract}

\tableofcontents

\section{Introduction}
A now-fully emergent field of data analysis, is that of regression and resampling. Included as a subset of the field of machine learning, regression is widely used as tool of prediction. Together with resampling we are offered ways of estimating the error of our models.

One common way to investigate the efficacy of a model is to use the Franke function\cite{franke1979critical}. This function has complexities which makes it suitable to use as a testing ground for machine learning. After studying the noise deependency on the quality of fit $R^2$, we moved on to investigating how the data fitting would behave for real terrain data.

We will start of by introducing the different regression methods, then resampling techiniques before giving a brief description of the implementation. After that we will present the results in an orderly fashion, before delving into the discussion and conclusion of how we can choose the optimum fit parameters for the different regression methods and how well the differet methods reproduce the data. Whitout further a due, let us start of by introducing some basic notation on linear regression.

\section{Methods and theory}
Let us start of by summing up notational conventions. We will denote a \textit{case} or \textit{sample} by $x_i$, where $i=0,\cdots,n$. This will have a corresponding \textit{response} or \textit{outcome} $y_i = f(x_i)$. We typically want to estimate $f$ by some approximate function $\tilde{f}$. As we shall see, one method is by performing linear regression.

\subsection{Linear regression}
As mentioned, linear regression is a method of performing a fit of parameters $x_i$ to a given data set $y_i$. From this we wish to minimize the error between the \textit{response} or \textit{real} data points $y_i$, and some approximation $\tilde{y}_i=\tilde{f}(x_i)$. This defines our loss function, which we will seek to minimize. In linear regression this will be by building an \textit{design matrix} built from $p$ \textit{explanatory variables} or \textit{predictors} $\bm{x_i}=x_i^0+x_i^1+\dots+x_i^p$. The explanatory variables is simply the degrees of the polynomial we seek to approximate $f$ by. So far, we have not mentioned the variables which will be tuned to the model, the \textit{regression parameters} $\beta=(\beta_1,\dots,\beta_p)^T$. The keen-eyed reader will now notice that the predictors and regression parameters all are of equal number, and may have guessed that they form our linear approximation - this is true:
\begin{align*}
    \tilde{y}_i = \beta_0 x_{i0} + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i
\end{align*}
From this, we get a neat matrix equation,
\begin{align}
    \bm{y} = \beta \bm{X} + \bm{\epsilon}
    \label{eq:linreg_eq}
\end{align}
We see that our goal has immediately become to minimize $\beta$. The definition of the error vector $\bm{\epsilon}$ will be given later.

The simplest case of such a linear regression minimization problem is just a simple line \citep[ch. 3.1, p. 61]{james2013introduction}. However, this can be generalized greatly and we end up with the standard go-to method called \textit{Ordinary Least Squares}.

\subsubsection{Ordinary Least Squares regression(OLS)}
To begin our journey towards a solution of OLS, we start by defining the \textit{loss function} for OLS. 
\begin{align}
    \underset{\beta\in \mathbb{R}^{p}}{\text{min}}||\bm{X}\beta - \bm{y}||^2_2 = 
    \underset{\beta\in \mathbb{R}^{p}}{\text{min}}\sum^n_{i=1}\left(\bm{x}^T_i\beta - y_i\right)^2
    \label{eq:ols_constraint}
\end{align}
We see that the goal is to minimize the L$_2$ norm between the response $y_i$ and the fitted predictors $\bm{x}_i^T\beta$ \citep[ch. 4, p. 21]{2018arXiv180308823M}.

To find a solution to the loss function, we can start by observing the dimensions of equation \eqref{eq:linreg_eq},
\begin{align}
    \underbrace{\vphantom{\beta}\bm{y}}_{\mathbb{R}^{n}} = \underbrace{\vphantom{\beta}\bm{X}}_{\mathbb{R}^{n\times p}} \underbrace{\beta}_{\mathbb{R}^{p}}
    \label{eq:matrix_eq_solution}
\end{align}
We then see that in order to free $\beta$, we must multiply by the transposed of $\bm{X}$ such that we can take the inverse and solve for $\beta$.

But, before we can do that we need to define what the optimal $\beta$ vector is. We start by defining a function $Q(\beta)$ which gives us the squared error of the spread,
\begin{align*}
    Q(\beta) &= \sum^{n-1}_{i=0} (y_i - \tilde{y}_i)^2 \\
    &= (\bm{y} - \tilde{\bm{y}})^T (\bm{y} - \tilde{\bm{y}}) \\
\end{align*}
which is the matrix equation
\begin{align}
    Q(\beta) = (\bm{y} - \bm{X}\beta)^T (\bm{y} - \bm{X}\beta)
    \label{eq:ols_cost_function}
\end{align}
This equation is called the \text{cost function} \citep[p. 12]{2018arXiv180308823M}. Before moving on, we should not that we can generalize it to
\begin{align}
    Q(\bm{y}, g(\bm{x})) = \sum_i \left(y_i - g(\bm{x}) \right)
    \label{eq:reg_cost_func}
\end{align}
where $g(\bm{x})$ is some function for the predictor. In OLS, this is $\beta_i \bm{x}_i$.

The minimum of the cost function \eqref{eq:ols_cost_function} can be found by differentiating by $\beta$ and setting the result equal to zero. 
\begin{align*}
    \frac{\partial Q(\beta)}{\partial\beta_j} &= \frac{\partial}{\partial\beta_j}\sum^{n-1}_{i=0} (y_i - \beta_0 x_{i0} - \beta_1 x_{i1} - \dots \beta_{p-1} x_{i,p-1})^2 \\
    &= -2\sum^{n-1}_{i=0} x_{ij}(y_i - \beta_0 x_{i0} - \beta_1 x_{i1} - \dots \beta_{p-1} x_{i,p-1}) \\
\end{align*}
where the element $x_{ij}$ is picked out by the differentiation w.r.t. $\beta_j$. Performing all of the derivatives leaves us with the matrix equation,
\begin{align}
    \frac{\partial Q(\beta)}{\partial\beta} = X^T (\bm{y} - \bm{X}\beta)
    \label{eq:ols_minimization}
\end{align}
which we require to be zero in order to find an optimal $\beta$. From this the \textit{residues} or \textit{error vector} $\bm{\epsilon}$ is defined,
\begin{align}
    \bm{\epsilon} = \bm{y} - \tilde{\bm{y}} = \bm{y} - \bm{X}\beta
    \label{eq:ols_residues}
\end{align}
The residues is the error between the \textit{approximation} and the \textit{real} data, and is not to be confused with the inherent error in the data. The solution should now be obvious. We need to solve equation \eqref{eq:ols_minimization} for $Q(\beta)=0$,
\begin{align*}
    \bm{X}^T (\bm{y} - \bm{X}\beta) = 0.
\end{align*}
As alluded to in \eqref{eq:matrix_eq_solution}, we now need to multiply by the inverse of $\bm{X}^T\bm{X}$, which yields after splitting the parenthesis
\begin{align*}
    \left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T \bm{y} = \left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T\bm{X}\beta)
\end{align*}
And thus, the $\beta$-values is simply
\begin{align}
    \beta = \left(\bm{X}^T\bm{X}\right)^{-1}\left(\bm{X}^T\bm{X}\right)\bm{y}
    \label{eq:ols_beta_solution}
\end{align}
This solution does not account for errors in the measurements or responses. Say there is an $\sigma_i$ associated with each measured $y_i$, we will have to use the $\chi^2$ function as a our function to minimize \citep[see notes on regression]{morten-regression}. We will have to rescale our solution by $\bm{X} \rightarrow \bm{A}=\bm{X}/\bm{\Sigma}$ where $\Sigma$ is a diagonal matrix of every uncertainty $\sigma_i$.

\subsubsection{Ridge regression}
The idea of Ridge regression is to add a small term $\lambda$ to constraint,
\begin{align}
    \beta(\lambda) = \underset{\beta\in \mathbb{R}^{p}}{\text{min}}(||\bm{X}\beta - \bm{y}||^2_2 + \lambda||\beta||^2_2)
    \label{eq:ridge_constraint}
\end{align}
This amounts to finding a solution in parameters space $\beta$ which tangents a circle defined by the L$^2$ norm of the constraint \eqref{eq:ridge_constraint}.

As shown in \cite{van2015lecture}, the solution to the Ridge regression, is to replace the inverse in the OLS solution for $\beta$ \eqref{eq:ols_beta_solution}, by
\begin{align}
    \bm{X}^T\bm{X} \rightarrow \bm{X}^T\bm{X} + \lambda \bm{I}
    \label{eq:ridge-inverse-replacement}
\end{align}
with $\bm{I}$ as the identity matrix.

The idea of adding an $\lambda\bm{I}$ can aid in resolving issues surrounding singularities in the inverse which can be troublesome when dealing with a large parameter space $\beta$\cite[see notes on regression, p. 17]{morten-regression}.

\subsubsection{Lasso regression}
The idea behind the Lasso regression is similar to that of Ridge regression. However, it comes with a few of its own benefits, such that it tends to zero out $\beta$ coefficients. To see this, we can take a look at the constraint function,
\begin{align}
    \beta(\lambda) = \underset{\beta\in \mathbb{R}^{p}}{\text{min}}(||\bm{X}\beta - \bm{y}||^2_2 + \lambda||\beta||^2_1)
    \label{eq:lasso_constraint}
\end{align}
where we are taking the L$^1$ norm(popularly called Taxicab or Manhattan norm) in the $\lambda$ term, $\lambda||\beta||^2_1$. Since the L$^1$ norm is defined as
\begin{align*}
    ||\bm{a}||_1 = |a_0| + \dots + |a_{n-1}| = \sum^{n-1}_{i=0}|a_i|
\end{align*}
To visualize and better understand the difference between Ridge and Lasso regression, we can take a look at figure \ref{fig:norms}. Here we see the difference to be that the Ridge will tend to get more $\beta_i$ values as zero, due to the L$^1$ norm having a greater chance of \textit{"hitting"} the corners of the diamond in figure \ref{fig:l1_norm}.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/l1_norm.pdf}
        \caption{L$^1$ norm for Lasso.}
        \label{fig:l1_norm}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/l2_norm.pdf}
        \caption{L$^2$ norm for Ridge.}
        \label{fig:l2_norm}
    \end{subfigure}
    \caption{In the figure \ref{fig:l2_norm} we see how the search space for the Ridge regression, where we define a success when the $\beta$ parameter hits the circle. The search space for Lasso is viewed on the figure to the right \ref{fig:l1_norm}.}
    \label{fig:norms}
\end{figure}


\subsection{The Bias-Variance decomposition}
It is possible to define variance of the predicted values $\tilde{y}$. If we take the estimator of the general cost function for linear regression \eqref{eq:reg_cost_func},
\begin{align*}
    E[Q(g(\bm{x}))] = E\left[\sum_i (y_i - g(\bm{x_i}))^2 \right]
\end{align*}
it can be shown that we can decompose this into the the variance of $\bm{y}$, bias and noise\cite{2018arXiv180308823M,morten-regression}.
\begin{align}
    \text{MSE} = \text{Bias}^2 + \text{Var} + \text{Noise}
    \label{eq:bias-variance}
\end{align}
The bias is defined as
\begin{align}
    \mathrm{Bias}^2 = \sum_i (\tilde{y}_i - E[\bm{y}])^2,
    \label{eq:bias}
\end{align}
which is measure of how much the prediction deviates from the real data.

A note on the Noise is needed here before we move on. The noise is defined as the the variance in the real data, $\text{Noise}=\text{Var}(\epsilon_\mathrm{noise})$. Since we cannot know the noise beforehand, this will be gobbled up into the bias, as that is a measure of the deviation in real data and prediction. Since the noise is irreducible in the sense we cannot remove the initial noise in the data, the MSE will never lie below $\text{Var}(\epsilon)$ \citep[ch. 2, p. 34]{james2013introduction}.

The variance in \eqref{eq:bias-variance} is defined as the variance in in the prediction, $\tilde{\bm{y}}$.

The MSE in \eqref{eq:bias-variance} is the Mean Square Error is defined as,
\begin{align}
    \text{MSE} = \frac{1}{n} \sum^{n-1}_{i=0}(y_i - \tilde{y}_i)^2,
    \label{eq:mse}
\end{align}
and is simply the averaged squared errors.

% \subsection{Mean Square Error(MSE)}
% The mean square error, popularly called MSE is a function that 
\subsection{\texorpdfstring{$R^2$}{R1} score}
A assessment of the fit quality is given by the $R^2$ score,
\begin{align}
    R^2(\bm{y},\tilde{\bm{y}}) = 1 - \frac{\sum_{i=0}^{n-1}(y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n-1}(y_i - \bar{\bm{y}})^2}
    \label{eq:r2_score}
\end{align}
with $\bar{\bm{y}}$ defined as 
\begin{align*}
    \bar{\bm{y}} = \frac{1}{n}\sum^{n-1}_{i=0} y_i
\end{align*}


\subsection{Determining the best fitting procedure}
An ad-hoc method of determining the best fitting procedure will be used by looking at,
\begin{align}
    \mathrm{Score} = \sqrt{(1-R^2)^2 + (\mathrm{MSE})^2}
    \label{eq:opt_fit}
\end{align}

\subsection{Bootstrapping}
Bootstrapping is a resampling technique particularly useful for small datasets or observables with difficult-to-asses statistics\cite{efron1994introduction}. The bootstrap technique is inspired by the Jackknife method\cite{efron1981}, but differs since we randomly selects a $n$ new samples with replacement of the original dataset $\{y_i\}$ a total of $N_\mathrm{bs}$ times. We use the $N_\mathrm{bs}$ samples to find the MSE, variance and bias. 

A basic for bootstrap algorithm can be set up as following,
\begin{algorithm}[H]
    \caption{Bootstrap}
    \label{alg:bootstrap}
    \begin{algorithmic}[1]
        \State Split the dataset $\bm{X}_{\mathcal{L}}=\{y_i,\bm{x}_i\}$ into a training set and a test set(or holdout set), $\bm{X}_{\mathcal{L},\mathrm{train}}$ and $\bm{X}_{\mathcal{L},\mathrm{test}}$.
        \For{$N_\mathrm{bs}$ times}
            \State Build a new data set $\bm{X}_{\mathcal{L},\mathrm{bs}}=\{y_{i,\mathrm{bs}},\bm{x}_{i,\mathrm{bs}}\}$ with $n$ randomly drawn with replacement values, from the training set $\bm{X}_{\mathcal{L},\mathrm{train}}$.
            \State Fit the bootstrapped dataset to a given model.
            \State Evaluate the model on on the test set $\bm{X}_{\mathcal{L},\mathrm{test}}$.
            \State Store relevant results.
        \EndFor
        \State Perform final statistics on either the collection of bootstrapped datasets and/or take the averages on the stored results. Depending on the type of variable you are sampling, you might need to use latter one.
    \end{algorithmic}
\end{algorithm}

\subsection{\texorpdfstring{$k$}{k}-fold Cross Validation}
The Cross Validation(CV) is a resampling technique which is similar in thought to the Jackknife method, but differs at a few critical places. Details on the $k$-fold CV method can be found in the book An Introduction to Statistical learning by \citet[ch. 5.1]{james2013introduction}, and can be summarized as following,
\begin{algorithm}[H]
    \caption{$k$-fold Cross Validation}
    \label{alg:kfcv}
    \begin{algorithmic}[1]
        \State Shuffle the dataset $\bm{X}_\mathcal{L}=\{y_i,\bm{x}_i\}$.
        \State Split the data set into training and test data, $\bm{X}_{\mathcal{L},\mathrm{train}}$ and $\bm{X}_{\mathcal{L},\mathrm{test}}$.
        \State Split the training data into $k$ \textit{folds}. $\bm{X}_{\mathcal{L},\mathrm{train}}\rightarrow \{\bm{X}_{\mathcal{L},i_k}\}$
        \For{each $i_k$ in the $k$ folds}
            \State Select the sets $\tilde{\bm{X}}_{\mathcal{L}} = \bm{X}_{\mathcal{L}} / \{\bm{X}_{\mathcal{L},i_k}\}$ as training sets, excluding the $k$'th set which is used a holdout set. Note: in order to evaluate the Bias-Variance score \eqref{eq:bias-variance}, the model will not be compared on the $i_k$ test set.
            \State Fit the $\tilde{\bm{X}}_{\mathcal{L}}$ to the model which is being used.
            \State Evaluate the $\tilde{\bm{X}}_{\mathcal{L}}$ on the test set $\bm{X}_{\mathcal{L},\mathrm{test}}$, and perform necessary statistics.
        \EndFor
        \State Summarize the evaluations and statistics gained from the $k$ folds.
    \end{algorithmic}
\end{algorithm}

The Cross Validation comes in many different flavors. Among popular ones are the Monte-Carlo Cross Validation(MCCV). The idea is the same, except that we randomly select the $i_k$'th set to leave out and run this $N_{MC}$ times instead of just $k$ times. The samples selected each time are selected without replacement, but one can have the same samples over many Monte Carlo iterations. E.g. the indices of the first set can be $12, 32, 56, \bm{42}, 94, 83$, and $49, 2, 59, \bm{42}, 51, 73$, where $\bm{42}$ repeats.

\section{Implementation}
The full implementation can be found at \cite{github-repo}. The OLS and Ridge regressions have been implemented, as well as the resampling methods Bootstrap, $k$-fold CV and MCCV. When taking the inverse in OLS regression \eqref{eq:ols_beta_solution} and the inverse in Ridge regression \eqref{eq:ridge-inverse-replacement} in order to find the $\beta$ coefficients a Singular Value Decomposition were used. 

Using scikit-learn\cite{scikit-learn} methods for OLS, Ridge and Lasso regression were implemented, as well as $k$-fold cross validation.

\section{Results}
The results presented are split in two section: the results governing method verification using the Franke function\eqref{eq:franke_function} and methods modeling terrain data as retrieved from \texttt{https://earthexplorer.usgs.gov/}.

\subsection{Verification through the Franke function}
The Franke function $F(x,y)$\eqref{eq:franke_function} maps coordinates $x$, $y$ to a $z$-coordinate. Such a mapping for points $x=[0,1]$, $y=[0,1]$ can be viewed in figure \ref{fig:franke_function}.
\begin{figure}
    \centering
    \includegraphics[scale=1.0]{../fig/franke_function.pdf}
    \caption{The mapping of $x=[0,1]$, $y=[0,1]$ through the Franke function \eqref{eq:franke_function}.}
    \label{fig:franke_function}
\end{figure}

% FRANKE FUNC HEAT MAP
In figure \ref{fig:mse_r2_heatmaps_ff_lasso}, we can observe the $R^2$-score and MSE and how Lasso regression behaves for different levels of noise and $\lambda$-parameters. The same can be seen for Lasso in figure \ref{fig:mse_r2_heatmaps_ff_ridge}.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/franke3_lasso_r2_deg5_dtyperegression_heatmap.pdf}
        \caption{A plot of $R^2$ versus noise and $\lambda$}
        \label{fig:franke_heatmat_r2}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/franke3_lasso_mse_deg5_dtyperegression_heatmap.pdf}
        \caption{A plot of MSE versus noise and $\lambda$}
        \label{fig:franke_heatmat_mse}
    \end{subfigure}
    \caption{A plot of how MSE and $R^2$ varies as function of noise and $\lambda$ with Lasso regression.}
    \label{fig:mse_r2_heatmaps_ff_lasso}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/franke3_ridge_r2_deg5_dtyperegression_heatmap.pdf}
        \caption{A plot of $R^2$ versus noise and $\lambda$}
        \label{fig:franke_heatmat_r2_ridge}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/franke3_ridge_mse_deg5_dtyperegression_heatmap.pdf}
        \caption{A plot of MSE versus noise and $\lambda$}
        \label{fig:franke_heatmat_mse_ridge}
    \end{subfigure}
    \caption{A plot of how MSE and $R^2$ varies as function of noise and $\lambda$ with Ridge regression.}
    \label{fig:mse_r2_heatmaps_ff_ridge}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[scale=1.0]{../fig/bias_variance_total.pdf}
%     \caption{The Bias-Variance relation from bootstrapping the regression data from the Franke function.}
%     \label{fig:franke_function_total_bias_variance}
% \end{figure}

The bias-variance for the Franke function can be seen in figure \ref{fig:bias-var-franke}.

% FRANKE FUNC BIAS VARIANCE
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/franke3_bias_variance_tradeoff_bootstrap_var_ridge.pdf}
        \caption{Ridge}
        \label{fig:biasvar-ridge-franke}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/franke3_bias_variance_tradeoff_bootstrap_var_ols.pdf}
        \caption{OLS}
        \label{fig:biasvar-OLSfranke}
    \end{subfigure} \\
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/franke3_bias_variance_tradeoff_bootstrap_var_lasso.pdf}
        \caption{Lasso}
        \label{fig:biasvar-bs-franke}
    \end{subfigure}
    \caption{Variance-bias plots of the OLS, Ridge and Lasso.}
    \label{fig:bias-var-franke}
\end{figure}


% \begin{table}[H]
%     \centering
%     \begin{tabular}{l l l l l l} % 6 columns
%         \specialrule{.1em}{.05em}{.05em}
%         Score     & Degree    &$R^2$      & MSE                   & $\mathrm{Bias}^2$     & Var \\ \hline
%         $0.022$      & 5         & $0.98$    & $1.76\cdot 10^{-3}$   & $1.75\cdot 10^{-3}$   & $1.61\cdot 10^{-6}$\\
%         \specialrule{.1em}{.05em}{.05em}
%     \end{tabular}
%     \caption{Based on the different fitting procedures, these fit parameters were determined to be the best. The polynomial fitted was of degree $8$.}
%     \label{tab:opt_result}
% \end{table}
% Optimal score for 
% 'score': 0.021736874931834748, 'r2': 0.9783342056295437, 'mse': 0.0017564413167902263, 'bias': 0.0017489537578899248, 'var': 7.4875589003012656e-06, 'reg_type': 'ols', 'degree': 5,

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{../fig/frank3_bs_d5_beta_values.pdf}
    \caption{The $\beta$ parameters of the Franke function for polynomial degree 5, a noise level of $\mathcal{0.8571,\infty}$ and a $\alpha=10^{-3}$. The errors are the $95\%$-CI.}
    \label{fig:franke_beta_parmas}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{../fig/franke_function_best_fit.png}
    \caption{The optimal fit of the Franke function\eqref{eq:franke_function}, using OLS and polynomial of degree 5.}
    \label{fig:optimal_fit_franke_func}
\end{figure}



\subsection{Modeling terrain}
The terrain we have looked is from a region close to Stavanger in Norway, and can be viewed in figure \ref{fig:terrain_map_full} together with a subset of this region that can be viewed in the sub-figure, \ref{fig:terrain_patch}. We chose a grid of size $100 \times 250$. Further, when modeling the terrain, it was normalized by maximal height found in the total dataset.

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{../fig/terrain_data.png}
    \caption{Terrain data from a region close to Stavanger, Norway.}
    \label{fig:terrain_map_full}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{../fig/terrain_patch.pdf}
    \caption{A subset of the region found in figure \ref{fig:terrain_map_full}.}
    \label{fig:terrain_patch}
\end{figure}

The optimal fit method was found by looking at the $R^2$ and MSE values\eqref{eq:opt_fit}. The result was that the OLS method proved to be the most effective, as summed up in table \ref{tab:opt_ols_result}. The figure which these parameters replicates in terms of terrain features can be viewed in figure \ref{fig:terrain_optimal_fit_ols}. An example of the optimal fit of the Lasso different Lasso models can be found in table \ref{tab:opt_lasso_result}.
\begin{table}[H]
    \centering
    \begin{tabular}{l l l l l l l} % 6 columns
        \specialrule{.1em}{.05em}{.05em}
        Score     & Degree    &$R^2$      & MSE                   & $\mathrm{Bias}^2$     & Var                   & $\lambda$ \\ \hline
        $0.12$      & $8$         & $0.88$    & $1.72\cdot 10^{-3}$   & $1.72\cdot 10^{-3}$   & $1.61\cdot 10^{-6}$   & $10^{-4}$ \\
        \specialrule{.1em}{.05em}{.05em}
    \end{tabular}
    \caption{Based on the different fitting procedures, these fit parameters were determined to be the best. The polynomial fitted was of degree $8$.}
    \label{tab:opt_lasso_result}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{l l l l l l} % 6 columns
        \specialrule{.1em}{.05em}{.05em}
        Score     & Degree    &$R^2$      & MSE                   & $\mathrm{Bias}^2$     & Var                    \\ \hline
        $9.0\cdot{-2}$      & $6$         & $0.91$    & $1.26\cdot 10^{-3}$   & $1.26\cdot 10^{-3}$   & $2.93\cdot 10^{-6}$   \\
        \specialrule{.1em}{.05em}{.05em}
    \end{tabular}
    \caption{Based on the different fitting procedures, these fit parameters were determined to be the best. The polynomial fitted was of degree $6$.}
    \label{tab:opt_ols_result}
\end{table}

% OLS
% {'score': 0.09024518915031703, 'r2': 0.9097636023691074, 'mse': 0.0012596457342964624, 'bias': 0.001256713308784061, 'var': 2.9324255124013686e-06, 'noise': 0.001256713308784061, 'reg_type': 'ols', 'degree': 6, 'beta': array([ 7.95824708e-01,  5.37249929e-04,  6.15513938e-03, -2.08542751e-04,
%         2.11857159e-06,  5.98614588e-05,  4.16095330e-06, -4.02939131e-06,
%         7.89545480e-06, -1.30379909e-05, -3.32149319e-08,  3.08213815e-08,
%         1.38111958e-09, -1.29121948e-07,  3.18378597e-07,  1.19263313e-10,
%        -3.57825641e-11, -3.67891193e-10,  5.83624735e-10,  6.46140823e-10,
%        -2.95508019e-09, -1.58929619e-13, -1.18643065e-13,  1.21616184e-12,
%        -1.33950779e-12, -6.44745044e-13, -1.39629121e-12,  9.64220510e-12])}

\begin{table}[H]
    \centering
    \begin{tabular}{r r r r}
        \specialrule{.1em}{.05em}{.05em}
        $7.95\cdot 10^{-1}$ & $ 5.37\cdot 10^{-4}$ & $ 6.15\cdot 10^{-3}$ & $-2.08\cdot 10^{-4}$ \\
        $2.11\cdot 10^{-6}$ & $ 5.98\cdot 10^{-5}$ & $ 4.16\cdot 10^{-6}$ & $-4.02\cdot 10^{-6}$ \\
        $7.89\cdot 10^{-6}$ & $-1.30\cdot 10^{-5}$ & $-3.32\cdot 10^{-8}$ & $ 3.08\cdot 10^{-8}$ \\
        $1.38\cdot 10^{-9}$ & $-1.29\cdot 10^{-7}$ & $ 3.18\cdot 10^{-7}$ & $ 1.19\cdot 10^{-10}$ \\
        $-3.57\cdot 10^{-11}$ & $-3.67\cdot 10^{-10}$ & $ 5.83\cdot 10^{-10}$ & $ 6.46\cdot 10^{-10}$ \\
        $-2.95\cdot 10^{-9}$ & $-1.58\cdot 10^{-13}$ & $-1.18\cdot 10^{-13}$ & $ 1.21\cdot 10^{-12}$ \\
        $-1.33\cdot 10^{-12}$ & $-6.44\cdot 10^{-13}$ & $-1.39\cdot 10^{-12}$ & $ 9.64\cdot 10^{-12}$ \\
        \specialrule{.1em}{.05em}{.05em}
    \end{tabular}
    \caption{The optimal fit $\beta$ parameters as seen for OLS for a polynomial of degree 6. The $\beta$ parameters are on row-column order.}
    \label{tab:opt_beta_params_ols}
\end{table}


% LASSO
% {'score': 0.12313261670539472, 'r2': 0.8768796483619059, 'mse': 0.0017379036882720093, 'bias': 0.0017361576915235599, 'var': 1.745996748449357e-06, 'alpha': 0.0001, 'noise': 0.0017361576915235599, 'reg_type': 'lasso', 'degree': 8, 'beta': array([ 8.40715735e-01, -3.61088984e-03,  2.62194267e-03,  2.91964035e-06,
%         6.05572842e-06, -1.53966856e-05,  4.10830837e-08, -3.92724830e-08,
%         1.40408026e-08, -3.27041482e-08,  1.44858027e-10, -1.93110027e-11,
%        -1.44737965e-09,  1.71224811e-09,  1.06400910e-09,  3.58442520e-13,
%         5.86756339e-13, -5.28444955e-12, -9.95743762e-12,  2.66172231e-11,
%         7.48607981e-12,  2.87673941e-16,  3.25493281e-15, -2.41427363e-15,
%        -4.82465969e-14, -4.94354540e-14,  2.41163093e-13, -2.94634399e-14,
%        -5.25734294e-18,  4.00592037e-18,  9.04326862e-17, -6.09334625e-17,
%        -2.92895511e-16, -3.61807113e-16,  1.50225626e-15, -1.21101820e-15,
%        -5.03333070e-20, -8.86697703e-20,  7.38543029e-19,  5.47575032e-19,
%        -7.92504796e-20, -1.80177562e-18, -4.85806104e-18,  3.85251545e-18,
%        -1.67008095e-17])}


The optimal fit of the terrain data using can be seen in figure \ref{fig:terrain_optimal_fit_ols}. The best regression method resulted in being OLS regression. The Beta parameters of this fit can seen in figure \ref{tab:opt_beta_params_ols}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{../fig/lasso_fit_deg8.png}
    \caption{The optimal fit of the terrain data using the Lasso method, for a polynomial of degree 8}
    \label{fig:terrain_optimal_fit_lasso}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{../fig/ols_optimal_fit.png}
    \caption{The optimal fit of the terrain data using the OLS method, for a polynomial of degree 6.}
    \label{fig:terrain_optimal_fit_ols}
\end{figure}


The Bias-Variance of the different terrain fits can be seen in figure \ref{fig:bias-var-terrain}.

% TERRAIN FUNC BIAS VARIANCE
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/terrain1_bias_variance_tradeoff_kfoldcv_var_ols.pdf}
        \caption{The bias-variance trade off for different polynomial degrees in the fit function OLS on the terrain data.}
        \label{fig:terrain-bias-var-ols}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/terrain1_bias_variance_tradeoff_kfoldcv_var_ridge.pdf}
        \caption{The bias-variance trade off for different polynomial degrees in the fit function Ridge on the terrain data.}
        \label{fig:terrain-bias-var-ridge}
    \end{subfigure} \\
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{../fig/terrain1_bias_variance_tradeoff_kfoldcv_var_lasso.pdf}
        \caption{The bias-variance trade off for different polynomial degrees in the fit function Lasso on the terrain data.}
        \label{fig:terrain-bias-var-lasso}
    \end{subfigure}
    \caption{Variance-bias plots of the OLS, Ridge and Lasso.}
    \label{fig:bias-var-terrain}
\end{figure}

% TERRAIN BETA PARAMETERS
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{../fig/terrain1_beta_values_d8_noise00000_alpha00001_bootstrap.pdf}
    \caption{The $\beta$ values for the optimal fit as seen in \ref{fig:terrain_optimal_fit_lasso}, plotted with their $95\%$-CI.}
    \label{fig:terrain_optimal_fit_params}
\end{figure}


% ridge_optimal_fit
% lasso_optimal_fit


\section{Discussion, conclusion and final thoughts}
We have now taken a deep dive into the world of linear regression, and it is time to reemerge at the surface and look at what we have learned. We started this journey by using the Franke function as a tool to test the OLS-, Ridge- and Lasso- regression and $k$-fold CV and bootstrap resampling. What we learned was that the efficacy Ridge and Lasso is greatly dependent on the noise of the data and their $\lambda$ values, as seen in the figures of \ref{fig:mse_r2_heatmaps_ff_ridge} and in \ref{fig:mse_r2_heatmaps_ff_lasso}. When looking at figure \ref{fig:mse_r2_heatmaps_ff_lasso}, we also discover that the $R^2$ value as rather low - never reaches above $0.02$. This might indicate that the regression analysis is incorrect for what we have found.

% However, as seen in some of the data, there appear to be an issue with the Ridge regression in when looking at the bias-variance relation in figure \ref{fig:terrain-bias-var-ridge}.

The optimal model which is seen in figure \ref{fig:terrain_optimal_fit_ols} appears to replicate the terrain quite well. In the table \ref{tab:opt_beta_params_ols} we see that the $\beta$ parameters stretches from around $10^{-1}$ to $10^{-13}$, which of course explains the minuscule $\beta$ parameters found seen in the figure \ref{fig:franke_beta_parmas}. The $\beta$ parameters were computed using resampling. As for why the Lasso and Ridge did not outperform OLS for the terrain data, might be explained by the fact that the data patch was not \textit{noise} enough. A better approach to investigate this, would be to randomly sample a set of different terrain patches, and test on each of them to see what gives the best result.

Further, the choice of optimal model through the equation \eqref{eq:opt_fit} is a rather ad-hoc approach. In the future, an better approach would be to perhaps include the errors in the $\beta$ parameters in the choice of optimal model.

Personally I would have have needed more time to iron out kinks of the code, and polish what I've written even further. Particularly, I would like to take a closer look at the Lasso regression and figure out why it delivers such low $R^2$ scores in figure \ref{fig:mse_r2_heatmaps_ff_lasso}. I would also have liked investigate how $MC-CV$ behaves in greater detail as no results for $MC-CV$ were presented here. A more detailed investigation into the resampling techniques would also be interesting. I had more time, a verification of code would also have been implemented more rigorously using unit tests cross checking of data against the methods of SciKit Learn. The dependency of the size of the data set is also something which would have been interesting to probe. Such as, how does the $R^2$ and MSE behave under different data sets. Further, investigating both the noise levels and the $\alpha$ values with a greater granularity would have been of interest as well.

To summarize, this has been a great exercise into the working of OLS, Ridge and Lasso, and we have seen that the OLS works best when there is little noise, while Lasso appears to for more complicated cases when theres greater variation in noise. Finally, an optimal model for some real terrain data was found. The OLS turned out to be the most accurate one, due to its ability to work around small to medium noise features in a data set. The final fit, as seen in figure\ref{fig:terrain_optimal_fit_params} with $R^2=0.91$ and MSE$=1.26\cdot 10^{-3}$, which is an acceptable fit considering the scope of this report.

\begin{appendices}
\subsection{The Franke Function} \label{sec:appendix}
The Franke function as given in \cite{franke1979critical},
\begin{align}
    F(x,y) &= 0.75\exp\left(-\left(0.25\left(9x-2\right)^2\right) - 0.25\left(\left(9y-2\right)^2\right)\right) \\
    &= 0.75\exp\left(-\frac{1}{49}\left(\left(9x+1\right)^2\right) - 0.1\left(9y+1\right)\right) \\
    &= 0.5\exp\left(-\left(9x-7\right)^2/4.0 - 0.25\left(\left(9y-3\right)^2\right)\right) \\
    &= -0.2\exp\left(-\left(9x-4\right)^2 - \left(9y-7\right)^2\right)
    \label{eq:franke_function}
\end{align}
\end{appendices}

\bibliography{bib/bibliography.bib}
\bibliographystyle{plainnat}


\end{document}