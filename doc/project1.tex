\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
% For figures and graphics'n stuff
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{color}
\usepackage{float}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{bm}

% Correct way for typing C++
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}

% For cmd line arguments
\usepackage{listings}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt}


\lstdefinestyle{custom-pro-file}{
  % belowcaptionskip=1\baselineskip,
  % frame=L,
  % xleftmargin=\parindent,
  % language=C++,
  basicstyle=\footnotesize\ttfamily,
  % commentstyle=\itshape\color{green!40!black},
  % keywordstyle=\bfseries\color{purple!40!black},
  identifierstyle=\color{black},
}


% For proper citations
% \usepackage[round, authoryear]{natbib}
\usepackage[numbers, authoryear]{natbib} 

% For color
\hypersetup{colorlinks=true,linkcolor=blue, linktocpage}

% For fixing large table height
\usepackage{a4wide}

\title{Machine Learning and Terrain data}
\author{Mathias M. Vege}

\date{\today}
\begin{document}
\maketitle

\begin{abstract}
None
\end{abstract}

\tableofcontents

\section{Introduction}
A now-fully emergent field of data analysis, is that of regression and resampling. Included as a subset of the field of machine learning, regression is widely used as tool of prediction. Together with resampling we are offered ways of estimating the error of our models.

One common way to investigate the efficacy of a model is to use the Franke function\cite{franke1979critical}.

\section{Methods and theory}
Let us start of by summing up notational conventions. We will denote a \textit{case} or \textit{sample} by $x_i$, where $i=0,\cdots,n$. This will have a corresponding \textit{response} or \textit{outcome} $y_i = f(x_i)$. We typically want to estimate $f$ by some approximate function $\tilde{f}$. As we shall see, one method is by performing linear regression.

\subsection{Linear regression}
As mentioned, linear regression is a method of performing a fit of parameters $x_i$ to a given data set $y_i$. From this we wish to minimize the error between the \textit{response} or real data points $y_i$, and some approximation $\tilde{y}_i=\tilde{f}(x_i)$. This defines our loss function, which we will seek to minimize. In linear regression this will be by building an \textit{design matrix} built from $p$ \textit{explanatory variables} or \textit{predictors} $\bm{x_i}=x_i^0+x_i^1+\dots+x_i^p$. The explanatory variables is simply the degrees of the polynomial we seek to approximate $f$ by. So far, we have not mentioned the variables which will be tuned to the model, the \textit{regression parameters} $\beta=(\beta_1,\dots,\beta_p)^T$. The keen-eyed reader will now notice that the predictors and regression parameters all are of equal number, and may have guessed that they form our linear approximation - this is true:
\begin{align*}
    \tilde{y}_i = \beta_0 x_{i0} + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i
\end{align*}
From this, we get a neat matrix equation,
\begin{align}
    \bm{y} = \beta \bm{X} + \bm{\epsilon}
    \label{eq:linreg_eq}
\end{align}
We see that our goal has immediately become to minimize $\beta$. The definition of the error vector $\bm{\epsilon}$ will be given later.

The simplest case of such a linear regression minimization problem is just a simple line\citep[ch. 3.1, p. 61]{james2013introduction}. However, this can be generalized greatly and we end up with the standard go-to method called \textit{Ordinary Least Squares}.

\subsubsection{Ordinary Least Squares regression(OLS)}
To begin our journey towards a solution of OLS, we start by defining the \textit{loss function} for OLS. 
\begin{align}
    \underset{\beta\in \mathbb{R}^{p}}{\text{min}}||\bm{X}\beta - \bm{y}||^2_2 = 
    \underset{\beta\in \mathbb{R}^{p}}{\text{min}}\sum^n_{i=1}\left(\bm{x}^T_i\beta - y_i\right)^2
\end{align}
We see that the goal is to minimize the L$_2$ norm between the response $y_i$ and the fitted predictors $\bm{x}_i^T\beta$ \citep[ch. 4, p. 21]{2018arXiv180308823M}.

To find a solution to the loss function, we can start by observing the dimensions of equation \eqref{eq:linreg_eq},
\begin{align}
    \underbrace{\vphantom{\beta}\bm{y}}_{\mathbb{R}^{n}} = \underbrace{\vphantom{\beta}\bm{X}}_{\mathbb{R}^{n\times p}} \underbrace{\beta}_{\mathbb{R}^{p}}
\end{align}
We then see that in order to free $\beta$, we must multiply by the transposed of $\bm{X}$ such that we can take the inverse and solve for $\beta$.

But, before we can do that we need to define what the optimal $\beta$ vector is. We start by defining a function $Q(\beta)$ which gives us the squared error of the spread,
\begin{align*}
    Q(\beta) &= \sum^{n-1}_{i=0} (y_i - \tilde{y}_i)^2 \\
    &= (\bm{y} - \tilde{\bm{y}})^T (\bm{y} - \tilde{\bm{y}}) \\
\end{align*}
which is the matrix equation
\begin{align}
    Q(\beta) = (\bm{y} - \bm{X}\beta)^T (\bm{y} - \bm{X}\beta)
    \label{eq:ols_minimization_function}
\end{align}
The minimum of this function can be found by differentiating by $\beta$ and setting the result equal to zero. 
\begin{align*}
    \frac{\partial Q(\beta)}{\partial\beta_j} &= \frac{\partial}{\partial\beta_j}\sum^{n-1}_{i=0} (y_i - \beta_0 x_{i0} - \beta_1 x_{i1} - \dots \beta_{p-1} x_{i,p-1})^2 \\
    &= -2\sum^{n-1}_{i=0} x_{ij}(y_i - \beta_0 x_{i0} - \beta_1 x_{i1} - \dots \beta_{p-1} x_{i,p-1}) \\
\end{align*}
where the element $x_{ij}$ is picked out by the differentiation w.r.t. $\beta_j$. Performing all of the derivatives leaves us with the matrix equation,
\begin{align}
    \frac{\partial Q(\beta)}{\partial\beta} = X^T (\bm{y} - \bm{X}\beta) = 0
    \label{eq:ols_minimization}
\end{align}
which we require to be zero in order to find an optimal $\beta$. From this the \textit{residues} or \textit{error vector} $\bm{\epsilon}$ is defined,
\begin{align}
    \bm{\epsilon}
    \label{eq:ols_residues}
\end{align}

\begin{align}
    \bm{y} = \beta \bm{X} + \bm{\epsilon}
\end{align}

\subsubsection{Ridge regression}
\subsubsection{Lasso regression}

\subsection{Mean Square Error(MSE)}
The mean square error, popularly called MSE is a function that 
\subsection{\texorpdfstring{$R^2$}{R1} score}
\subsection{Bootstrapping}
\subsection{\texorpdfstring{$k$}{k}-fold Cross Validation}

\section{Implementation}
\section{Results}
\section{Discussion and conclusion}
\cite{scikit-learn}

\section{Appendix} \label{sec:appendix}
\subsection{The Franke Function}

\bibliography{bib/bibliography.bib}
\bibliographystyle{plain}


\end{document}