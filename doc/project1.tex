\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
% For figures and graphics'n stuff
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{color}
\usepackage{float}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{bm}

% Correct way for typing C++
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}

% For cmd line arguments
\usepackage{listings}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt}


\lstdefinestyle{custom-pro-file}{
  % belowcaptionskip=1\baselineskip,
  % frame=L,
  % xleftmargin=\parindent,
  % language=C++,
  basicstyle=\footnotesize\ttfamily,
  % commentstyle=\itshape\color{green!40!black},
  % keywordstyle=\bfseries\color{purple!40!black},
  identifierstyle=\color{black},
}


% For proper citations
% \usepackage[round, authoryear]{natbib}
\usepackage[numbers, authoryear]{natbib} 

% For color
\hypersetup{colorlinks=true,linkcolor=blue, linktocpage}

% For fixing large table height
\usepackage{a4wide}

\title{Machine Learning and Terrain data}
\author{Mathias M. Vege}

\date{\today}
\begin{document}
\maketitle

\begin{abstract}
None
\end{abstract}

\tableofcontents

\section{Introduction}
A now-fully emergent field of data analysis, is that of regression and resampling. Included as a subset of the field of machine learning, regression is widely used as tool of prediction. Together with resampling we are offered ways of estimating the error of our models.

One common way to investigate the efficacy of a model is to use the Franke function\cite{franke1979critical}.

\section{Methods and theory}
Let us start of by summing up notational conventions. We will denote a \textit{case} or \textit{sample} by $x_i$, where $i=0,\cdots,n$. This will have a corresponding \textit{response} or \textit{outcome} $y_i = f(x_i)$. We typically want to estimate $f$ by some approximate function $\tilde{f}$. As we shall see, one method is by performing linear regression.

\subsection{Linear regression}
As mentioned, linear regression is a method of performing a fit of parameters $x_i$ to a given data set $y_i$. From this we wish to minimize the error between the \textit{response} or real data points $y_i$, and some approximation $\tilde{y}_i=\tilde{f}(x_i)$. This defines our loss function, which we will seek to minimize. In linear regression this will be by building an \textit{design matrix} built from $p$ \textit{explanatory variables} or \textit{predictors} $\bm{x_i}=x_i^0+x_i^1+\dots+x_i^p$. The explanatory variables is simply the degrees of the polynomial we seek to approximate $f$ by. So far, we have not mentioned the variables which will be tuned to the model, the \textit{regression parameters} $\beta=(\beta_1,\dots,\beta_p)^T$. The keen-eyed reader will now notice that the predictors and regression parameters all are of equal number, and may have guessed that they form our linear approximation - this is true:
\begin{align*}
    \tilde{y}_i = \beta_0 x_{i0} + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i
\end{align*}
From this, we get a neat matrix equation,
\begin{align}
    \bm{y} = \beta \bm{X} + \bm{\epsilon}
    \label{eq:linreg_eq}
\end{align}
We see that our goal has immediately become to minimize $\beta$. The definition of the error vector $\bm{\epsilon}$ will be given later.

The simplest case of such a linear regression minimization problem is just a simple line\citep[ch. 3.1, p. 61]{james2013introduction}. However, this can be generalized greatly and we end up with the standard go-to method called \textit{Ordinary Least Squares}.

\subsubsection{Ordinary Least Squares regression(OLS)}
To begin our journey towards a solution of OLS, we start by defining the \textit{loss function} for OLS. 
\begin{align}
    \underset{\beta\in \mathbb{R}^{p}}{\text{min}}||\bm{X}\beta - \bm{y}||^2_2 = 
    \underset{\beta\in \mathbb{R}^{p}}{\text{min}}\sum^n_{i=1}\left(\bm{x}^T_i\beta - y_i\right)^2
\end{align}
We see that the goal is to minimize the L$_2$ norm between the response $y_i$ and the fitted predictors $\bm{x}_i^T\beta$ \citep[ch. 4, p. 21]{2018arXiv180308823M}.

To find a solution to the loss function, we can start by observing the dimensions of equation \eqref{eq:linreg_eq},
\begin{align}
    \underbrace{\vphantom{\beta}\bm{y}}_{\mathbb{R}^{n}} = \underbrace{\vphantom{\beta}\bm{X}}_{\mathbb{R}^{n\times p}} \underbrace{\beta}_{\mathbb{R}^{p}}
    \label{eq:matrix_eq_solution}
\end{align}
We then see that in order to free $\beta$, we must multiply by the transposed of $\bm{X}$ such that we can take the inverse and solve for $\beta$.

But, before we can do that we need to define what the optimal $\beta$ vector is. We start by defining a function $Q(\beta)$ which gives us the squared error of the spread,
\begin{align*}
    Q(\beta) &= \sum^{n-1}_{i=0} (y_i - \tilde{y}_i)^2 \\
    &= (\bm{y} - \tilde{\bm{y}})^T (\bm{y} - \tilde{\bm{y}}) \\
\end{align*}
which is the matrix equation
\begin{align}
    Q(\beta) = (\bm{y} - \bm{X}\beta)^T (\bm{y} - \bm{X}\beta)
    \label{eq:ols_minimization_function}
\end{align}
This equation is called the \text{cost function}\citep[p. 12]{2018arXiv180308823M}. The minimum of this function can be found by differentiating by $\beta$ and setting the result equal to zero. 
\begin{align*}
    \frac{\partial Q(\beta)}{\partial\beta_j} &= \frac{\partial}{\partial\beta_j}\sum^{n-1}_{i=0} (y_i - \beta_0 x_{i0} - \beta_1 x_{i1} - \dots \beta_{p-1} x_{i,p-1})^2 \\
    &= -2\sum^{n-1}_{i=0} x_{ij}(y_i - \beta_0 x_{i0} - \beta_1 x_{i1} - \dots \beta_{p-1} x_{i,p-1}) \\
\end{align*}
where the element $x_{ij}$ is picked out by the differentiation w.r.t. $\beta_j$. Performing all of the derivatives leaves us with the matrix equation,
\begin{align}
    \frac{\partial Q(\beta)}{\partial\beta} = X^T (\bm{y} - \bm{X}\beta)
    \label{eq:ols_minimization}
\end{align}
which we require to be zero in order to find an optimal $\beta$. From this the \textit{residues} or \textit{error vector} $\bm{\epsilon}$ is defined,
\begin{align}
    \bm{\epsilon} = \bm{y} - \tilde{\bm{y}} = \bm{y} - \bm{X}\beta
    \label{eq:ols_residues}
\end{align}
The solution should now be obvious. We need to solve equation \eqref{eq:ols_minimization} for $Q(\beta)=0$,
\begin{align*}
    \bm{X}^T (\bm{y} - \bm{X}\beta) = 0.
\end{align*}
As alluded to in \eqref{eq:matrix_eq_solution}, we now need to multiply by the inverse of $\bm{X}^T\bm{X}$, which yields after splitting the parenthesis
\begin{align*}
    \left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T \bm{y} = \left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T\bm{X}\beta)
\end{align*}
And thus, the $\beta$-values is simply
\begin{align}
    \beta = \left(\bm{X}^T\bm{X}\right)^{-1}\left(\bm{X}^T\bm{X}\right)\bm{y}
    \label{eq:ols_beta}
\end{align}
This solution does not account for errors in the measurements or responses. Say there is an $\sigma_i$ associated with each measured $y_i$, we will have to use the $\chi^2$ function as a our function to minimize\citep[see notes on regression]{morten-regression}. We will have to rescale our solution by $\bm{X} \rightarrow \bm{A}=\bm{X}/\bm{\Sigma}$ where $\Sigma$ is a diagonal matrix of every uncertainty $\sigma_i$.

\subsubsection{Ridge regression}
\subsubsection{Lasso regression}

\subection{The Bias-Variance decomposition}
It is possible to define variance of the predicted values $\tilde{y}$. If we take the estimator of our cost function,
\begin{align}
    E[Q(\beta)] = E\left[\right]
\end{align}

% \subsection{Mean Square Error(MSE)}
% The mean square error, popularly called MSE is a function that 
\subsection{\texorpdfstring{$R^2$}{R1} score}

\subsection{Bootstrapping}
\subsection{\texorpdfstring{$k$}{k}-fold Cross Validation}

\section{Implementation}
\cite{scikit-learn}
\section{Results}
\section{Discussion and conclusion}

\section{Appendix} \label{sec:appendix}
\subsection{The Franke Function}
As given in \cite{franke1979critical},
\begin{align}
    F(x,y) &= 0.75\exp\left(-\left(0.25\left(9x-2\right)^2\right) - 0.25\left(\left(9y-2\right)^2\right)\right) \\
    &= 0.75\exp\left(-\frac{1}{49}\left(\left(9x+1\right)^2\right) - 0.1\left(9y+1\right)\right) \\
    &= 0.5\exp\left(-\left(9x-7\right)^2/4.0 - 0.25\left(\left(9y-3\right)^2\right)\right) \\
    &= -0.2\exp\left(-\left(9x-4\right)^2 - \left(9y-7\right)^2\right)
    \label{eq:franke_function}
\end{align}

\bibliography{bib/bibliography.bib}
\bibliographystyle{plain}


\end{document}